"""
ä½¿ç”¨ LLM ä»ä¹¦ç±ä¸­æå–çŸ¥è¯†
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

from .schema import Entity, Relationship, EntityType, RelationType

console = Console()


# å®ä½“æå–æç¤ºè¯
ENTITY_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨åˆ†æã€Šç©·æŸ¥ç†å®å…¸ã€‹çš„çŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–é‡è¦çš„å®ä½“ã€‚

## å®ä½“ç±»å‹
1. æ¦‚å¿µ - æ€ç»´æ¡†æ¶ã€æŠ•èµ„ç†å¿µï¼ˆå¦‚ï¼šå¤šå…ƒæ€ç»´æ¨¡å‹ã€èƒ½åŠ›åœˆï¼‰
2. æ€ç»´æ¨¡å‹ - å…·ä½“çš„æ€ç»´å·¥å…·ï¼ˆå¦‚ï¼šé€†å‘æ€è€ƒã€å¤åˆ©æ•ˆåº”ï¼‰
3. åŸåˆ™ - æŠ•èµ„æˆ–ç”Ÿæ´»åŸåˆ™ï¼ˆå¦‚ï¼šå®‰å…¨è¾¹é™…ï¼‰
4. äººç‰© - ä¹¦ä¸­æåˆ°çš„é‡è¦äººç‰©
5. å…¬å¸ - æåˆ°çš„å…¬å¸
6. æ¡ˆä¾‹ - æŠ•èµ„æˆ–å•†ä¸šæ¡ˆä¾‹
7. å­¦ç§‘ - æ¶‰åŠçš„å­¦ç§‘é¢†åŸŸ
8. è®¤çŸ¥åè¯¯ - äººç±»è¯¯åˆ¤å¿ƒç†å­¦ä¸­çš„åè¯¯

## æ–‡æœ¬
{text}

## è¾“å‡ºè¦æ±‚
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ªå®ä½“åŒ…å« nameï¼ˆåç§°ï¼‰ã€typeï¼ˆç±»å‹ï¼‰ã€descriptionï¼ˆç®€çŸ­æè¿°ï¼‰ï¼š
```json
[
  {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "description": "ä¸€å¥è¯æè¿°"}}
]
```

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚æå–æœ€é‡è¦çš„å®ä½“ï¼ˆæœ€å¤š15ä¸ªï¼‰ã€‚"""


# å…³ç³»æå–æç¤ºè¯
RELATION_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å’Œå®ä½“åˆ—è¡¨ï¼Œæå–å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚

## å·²çŸ¥å®ä½“
{entities}

## æ–‡æœ¬
{text}

## å…³ç³»ç±»å‹
- è§£é‡Šï¼šAè§£é‡ŠBï¼ˆå¦‚ï¼šæ¿€åŠ±æœºåˆ¶ è§£é‡Š è”é‚¦å¿«é€’æ¡ˆä¾‹ï¼‰
- åº”ç”¨äºï¼šAåº”ç”¨äºB
- æºè‡ªï¼šAæºè‡ªBï¼ˆå¦‚ï¼šå¤åˆ©æ€ç»´ æºè‡ª å¯Œå…°å…‹æ—ï¼‰
- æ”¯æŒï¼šAæ”¯æŒBçš„è§‚ç‚¹
- åå¯¹ï¼šAåå¯¹B
- å¯¼è‡´ï¼šAå¯¼è‡´B
- å±äºï¼šAå±äºBï¼ˆå¦‚ï¼šæ¿€åŠ±æœºåˆ¶ å±äº è¯¯åˆ¤å¿ƒç†å­¦ï¼‰
- ç›¸å…³ï¼šAä¸Bç›¸å…³

## è¾“å‡ºè¦æ±‚
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ªå…³ç³»åŒ…å« sourceï¼ˆæºå®ä½“ï¼‰ã€targetï¼ˆç›®æ ‡å®ä½“ï¼‰ã€typeï¼ˆå…³ç³»ç±»å‹ï¼‰ã€descriptionï¼ˆæè¿°ï¼‰ï¼š
```json
[
  {{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "type": "å…³ç³»ç±»å‹", "description": "æè¿°"}}
]
```

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚æå–æœ€é‡è¦çš„å…³ç³»ï¼ˆæœ€å¤š10ä¸ªï¼‰ã€‚"""


class LLMKnowledgeExtractor:
    """ä½¿ç”¨ LLM æå–çŸ¥è¯†ï¼ˆä¼˜åŒ–æˆæœ¬ç‰ˆï¼‰"""
    
    def __init__(self, llm, chunk_size: int = 3000):
        """
        Args:
            llm: LLM å®ä¾‹
            chunk_size: æ¯å—å¤§å°ï¼ˆå¢å¤§å¯å‡å°‘ API è°ƒç”¨æ¬¡æ•°ï¼‰
        """
        self.llm = llm
        self.chunk_size = chunk_size
        self.entities: Dict[str, Entity] = {}
        self.relationships: List[Relationship] = []
    
    def extract_from_file(self, file_path: str, max_chunks: int = 15) -> Tuple[Dict[str, Entity], List[Relationship]]:
        """ä»æ–‡ä»¶æå–çŸ¥è¯†"""
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        return self.extract_from_text(text, max_chunks)
    
    def extract_from_text(self, text: str, max_chunks: int = 30) -> Tuple[Dict[str, Entity], List[Relationship]]:
        """ä»æ–‡æœ¬æå–çŸ¥è¯†"""
        console.print("[bold cyan]ğŸ§  ä½¿ç”¨ LLM ä»ä¹¦ç±ä¸­æå–çŸ¥è¯†...[/bold cyan]")
        
        # 1. åˆ†å—
        chunks = self._split_into_chunks(text)
        console.print(f"ğŸ“– å…± {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå°†å¤„ç†å‰ {min(len(chunks), max_chunks)} ä¸ª")
        
        # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆé¿å… API è°ƒç”¨è¿‡å¤šï¼‰
        chunks_to_process = chunks[:max_chunks]
        
        # 2. é€å—æå–å®ä½“
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console
        ) as progress:
            task = progress.add_task("æå–å®ä½“...", total=len(chunks_to_process))
            
            for i, chunk in enumerate(chunks_to_process):
                try:
                    self._extract_entities_from_chunk(chunk, i)
                except Exception as e:
                    console.print(f"[yellow]å— {i} æå–å¤±è´¥: {e}[/yellow]")
                progress.update(task, advance=1)
        
        console.print(f"âœ… æå–åˆ° {len(self.entities)} ä¸ªå®ä½“")
        
        # 3. æå–å…³ç³»ï¼ˆä½¿ç”¨éƒ¨åˆ†æ–‡æœ¬ï¼‰
        console.print("\n[yellow]ğŸ”— æå–å®ä½“å…³ç³»...[/yellow]")
        self._extract_relationships(chunks_to_process[:10])
        console.print(f"âœ… æå–åˆ° {len(self.relationships)} ä¸ªå…³ç³»")
        
        # 4. æ·»åŠ æ ¸å¿ƒå…³ç³»
        self._add_core_relationships()
        
        return self.entities, self.relationships
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†æˆå—"""
        chunks = []
        
        # æŒ‰ç« èŠ‚åˆ†å‰²
        sections = re.split(r'\n===\s*.*?\s*===\n', text)
        
        for section in sections:
            section = section.strip()
            if len(section) < 100:
                continue
            
            # å¦‚æœç« èŠ‚å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(section) > self.chunk_size:
                paragraphs = section.split('\n\n')
                current_chunk = ""
                
                for para in paragraphs:
                    if len(current_chunk) + len(para) < self.chunk_size:
                        current_chunk += para + "\n\n"
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = para + "\n\n"
                
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
            else:
                chunks.append(section)
        
        return chunks
    
    def _extract_entities_from_chunk(self, chunk: str, chunk_idx: int):
        """ä»å•ä¸ªå—ä¸­æå–å®ä½“"""
        prompt = ENTITY_EXTRACTION_PROMPT.format(text=chunk[:2000])
        
        response = self._call_llm(prompt)
        entities = self._parse_entities(response)
        
        for e in entities:
            name = e.get('name', '').strip()
            if name and len(name) >= 2 and name not in self.entities:
                entity_type = self._map_entity_type(e.get('type', 'æ¦‚å¿µ'))
                self.entities[name] = Entity(
                    name=name,
                    entity_type=entity_type,
                    description=e.get('description', ''),
                    source_chapter=f"chunk_{chunk_idx}"
                )
    
    def _extract_relationships(self, chunks: List[str]):
        """æå–å…³ç³»ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¤šè½®æå– + å…±ç°åˆ†æï¼‰"""
        entity_names = list(self.entities.keys())
        
        # æ–¹æ³•1ï¼šåŸºäºå…±ç°å…³ç³»ï¼ˆåŒä¸€æ®µè½å‡ºç°çš„å®ä½“å¯èƒ½ç›¸å…³ï¼‰
        console.print("  ğŸ“Š åˆ†æå…±ç°å…³ç³»...")
        all_text = "\n\n".join(chunks)
        paragraphs = all_text.split('\n\n')
        
        cooccurrence = {}
        for para in paragraphs:
            entities_in_para = [e for e in entity_names if e in para and len(e) >= 2]
            # ä¸¤ä¸¤ç»„åˆ
            for i, e1 in enumerate(entities_in_para):
                for e2 in entities_in_para[i+1:]:
                    if e1 != e2:
                        pair = tuple(sorted([e1, e2]))
                        cooccurrence[pair] = cooccurrence.get(pair, 0) + 1
        
        # æ·»åŠ å…±ç°æ¬¡æ•°>=2çš„å…³ç³»
        for (e1, e2), count in sorted(cooccurrence.items(), key=lambda x: -x[1])[:30]:
            if count >= 2:
                self.relationships.append(Relationship(
                    source=e1,
                    target=e2,
                    relation_type=RelationType.RELATED_TO,
                    description=f"åœ¨ä¹¦ä¸­å…±åŒå‡ºç° {count} æ¬¡"
                ))
        
        # æ–¹æ³•2ï¼šè®© LLM ä¸“é—¨åˆ†ææ ¸å¿ƒå®ä½“çš„å…³ç³»
        console.print("  ğŸ§  LLM åˆ†ææ ¸å¿ƒå…³ç³»...")
        core_entities = entity_names[:30]  # å–å‰30ä¸ªæ ¸å¿ƒå®ä½“
        entity_list = ", ".join(core_entities)
        
        # ä½¿ç”¨ä¸“é—¨çš„å…³ç³»æå–æç¤ºè¯
        prompt = RELATION_EXTRACTION_PROMPT.format(
            entities=entity_list,
            text="\n\n".join(chunks[:3])[:3000]
        )
        
        response = self._call_llm(prompt)
        relations = self._parse_relations(response)
        
        for r in relations:
            source = r.get('source', '').strip()
            target = r.get('target', '').strip()
            
            if source in self.entities and target in self.entities and source != target:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                exists = any(
                    rel.source == source and rel.target == target 
                    for rel in self.relationships
                )
                if not exists:
                    rel_type = self._map_relation_type(r.get('type', 'ç›¸å…³'))
                    self.relationships.append(Relationship(
                        source=source,
                        target=target,
                        relation_type=rel_type,
                        description=r.get('description', '')
                    ))
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM"""
        if hasattr(self.llm, 'complete'):
            response = self.llm.complete(prompt)
            if hasattr(response, 'text'):
                return response.text
            return str(response)
        else:
            return self.llm.chat([{"role": "user", "content": prompt}])
    
    def _parse_entities(self, response: str) -> List[Dict]:
        """è§£æå®ä½“ JSON"""
        try:
            # å°è¯•æ‰¾åˆ° JSON æ•°ç»„
            match = re.search(r'\[[\s\S]*\]', response)
            if match:
                return json.loads(match.group())
        except json.JSONDecodeError:
            pass
        return []
    
    def _parse_relations(self, response: str) -> List[Dict]:
        """è§£æå…³ç³» JSON"""
        try:
            match = re.search(r'\[[\s\S]*\]', response)
            if match:
                return json.loads(match.group())
        except json.JSONDecodeError:
            pass
        return []
    
    def _map_entity_type(self, type_str: str) -> EntityType:
        """æ˜ å°„å®ä½“ç±»å‹"""
        type_map = {
            "æ¦‚å¿µ": EntityType.CONCEPT,
            "æ€ç»´æ¨¡å‹": EntityType.MENTAL_MODEL,
            "åŸåˆ™": EntityType.PRINCIPLE,
            "äººç‰©": EntityType.PERSON,
            "å…¬å¸": EntityType.COMPANY,
            "æ¡ˆä¾‹": EntityType.CASE_STUDY,
            "ä¹¦ç±": EntityType.BOOK,
            "å­¦ç§‘": EntityType.DISCIPLINE,
            "è®¤çŸ¥åè¯¯": EntityType.COGNITIVE_BIAS,
        }
        return type_map.get(type_str, EntityType.CONCEPT)
    
    def _map_relation_type(self, type_str: str) -> RelationType:
        """æ˜ å°„å…³ç³»ç±»å‹"""
        type_map = {
            "è§£é‡Š": RelationType.EXPLAINS,
            "åº”ç”¨äº": RelationType.APPLIES_TO,
            "æºè‡ª": RelationType.DERIVED_FROM,
            "ç›¸å…³": RelationType.RELATED_TO,
            "æ”¯æŒ": RelationType.SUPPORTS,
            "åå¯¹": RelationType.OPPOSES,
            "å±äº": RelationType.PART_OF,
            "å¯¼è‡´": RelationType.LEADS_TO,
            "å½±å“": RelationType.INFLUENCED_BY,
        }
        return type_map.get(type_str, RelationType.RELATED_TO)
    
    def _add_core_relationships(self):
        """æ·»åŠ æ ¸å¿ƒå…³ç³»"""
        core_relations = [
            ("æŸ¥ç†Â·èŠ’æ ¼", "å¤šå…ƒæ€ç»´æ¨¡å‹", "æ”¯æŒ", "èŠ’æ ¼æ˜¯å¤šå…ƒæ€ç»´æ¨¡å‹çš„æ ¸å¿ƒå€¡å¯¼è€…"),
            ("æŸ¥ç†Â·èŠ’æ ¼", "æ²ƒä¼¦Â·å·´è²ç‰¹", "ç›¸å…³", "é•¿æœŸåˆä½œä¼™ä¼´"),
            ("æŸ¥ç†Â·èŠ’æ ¼", "ä¼¯å…‹å¸Œå°”Â·å“ˆæ’’éŸ¦", "å±äº", "å‰¯è‘£äº‹é•¿"),
            ("å¤šå…ƒæ€ç»´æ¨¡å‹", "ç‰©ç†å­¦", "æºè‡ª", "å€Ÿé‰´ç‰©ç†å­¦æ€ç»´"),
            ("å¤šå…ƒæ€ç»´æ¨¡å‹", "å¿ƒç†å­¦", "æºè‡ª", "å€Ÿé‰´å¿ƒç†å­¦æ€ç»´"),
            ("å¤šå…ƒæ€ç»´æ¨¡å‹", "ç»æµå­¦", "æºè‡ª", "å€Ÿé‰´ç»æµå­¦æ€ç»´"),
        ]
        
        for source, target, rel_type, desc in core_relations:
            if source in self.entities and target in self.entities:
                self.relationships.append(Relationship(
                    source=source,
                    target=target,
                    relation_type=self._map_relation_type(rel_type),
                    description=desc
                ))

