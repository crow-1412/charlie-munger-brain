"""
å‘é‡æ£€ç´¢æ¨¡å—
ä½¿ç”¨ Embedding å®ç°è¯­ä¹‰æœç´¢
"""

import os
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from rich.console import Console
from rich.progress import track

console = Console()


@dataclass
class TextChunk:
    """æ–‡æœ¬å—"""
    text: str
    chapter: str
    index: int
    

class VectorStore:
    """å‘é‡å­˜å‚¨å’Œæ£€ç´¢"""
    
    def __init__(self, embedding_model: str = "dashscope"):
        """
        Args:
            embedding_model: ä½¿ç”¨çš„ embedding æ¨¡å‹
                - "dashscope": ä½¿ç”¨é€šä¹‰åƒé—® embeddingï¼ˆéœ€è¦ API Keyï¼‰
                - "local": ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ä½†è¾ƒæ…¢ï¼‰
        """
        self.embedding_model = embedding_model
        self.chunks: List[TextChunk] = []
        self.embeddings: Optional[np.ndarray] = None
        self.dimension = 0
        
        # åˆå§‹åŒ– embedding å‡½æ•°
        if embedding_model == "dashscope":
            self._init_dashscope_embedding()
        else:
            self._init_local_embedding()
    
    def _init_dashscope_embedding(self):
        """åˆå§‹åŒ–é€šä¹‰åƒé—® Embedding"""
        try:
            import dashscope
            from dashscope import TextEmbedding
            
            # ä»ç¯å¢ƒå˜é‡è·å– API Key
            api_key = os.getenv("DASHSCOPE_API_KEYS") or os.getenv("DASHSCOPE_API_KEY")
            if api_key:
                dashscope.api_key = api_key
            
            self.dimension = 1536  # text-embedding-v2 ç»´åº¦
            
            def embed_texts(texts: List[str]) -> np.ndarray:
                """æ‰¹é‡ç”Ÿæˆ embedding"""
                embeddings = []
                # DashScope æ¯æ¬¡æœ€å¤šå¤„ç† 25 ä¸ªæ–‡æœ¬
                batch_size = 25
                
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i + batch_size]
                    response = TextEmbedding.call(
                        model="text-embedding-v2",
                        input=batch
                    )
                    if response.status_code == 200:
                        for item in response.output['embeddings']:
                            embeddings.append(item['embedding'])
                    else:
                        console.print(f"[yellow]Embedding å¤±è´¥: {response.message}[/yellow]")
                        # å¤±è´¥æ—¶ç”¨é›¶å‘é‡å¡«å……
                        for _ in batch:
                            embeddings.append([0.0] * self.dimension)
                
                return np.array(embeddings, dtype=np.float32)
            
            self._embed_texts = embed_texts
            console.print("[green]âœ… ä½¿ç”¨é€šä¹‰åƒé—® Embedding (text-embedding-v2)[/green]")
            
        except Exception as e:
            console.print(f"[yellow]DashScope Embedding åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ°æœ¬åœ°æ¨¡å‹[/yellow]")
            self._init_local_embedding()
    
    def _init_local_embedding(self):
        """åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            
            console.print("[yellow]æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ï¼ˆé¦–æ¬¡å¯èƒ½éœ€è¦ä¸‹è½½ï¼‰...[/yellow]")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self.dimension = 384
            
            def embed_texts(texts: List[str]) -> np.ndarray:
                return self.model.encode(texts, show_progress_bar=False)
            
            self._embed_texts = embed_texts
            console.print("[green]âœ… ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹[/green]")
            
        except Exception as e:
            console.print(f"[red]æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
            raise
    
    def build_from_text(self, text: str, chunk_size: int = 500, chunk_overlap: int = 100):
        """ä»æ–‡æœ¬æ„å»ºå‘é‡ç´¢å¼•"""
        console.print("[cyan]ğŸ“š æ„å»ºå‘é‡ç´¢å¼•...[/cyan]")
        
        # 1. åˆ†å—
        self.chunks = self._split_text(text, chunk_size, chunk_overlap)
        console.print(f"  ğŸ“„ å…± {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
        
        # 2. ç”Ÿæˆ embedding
        console.print(f"  ğŸ”„ ç”Ÿæˆ Embeddingï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        texts = [chunk.text for chunk in self.chunks]
        
        # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
        batch_size = 100
        all_embeddings = []
        
        for i in track(range(0, len(texts), batch_size), description="ç”Ÿæˆå‘é‡"):
            batch = texts[i:i + batch_size]
            batch_embeddings = self._embed_texts(batch)
            all_embeddings.append(batch_embeddings)
        
        self.embeddings = np.vstack(all_embeddings)
        console.print(f"  âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼ç»´åº¦: {self.embeddings.shape}")
    
    def build_from_file(self, file_path: str, chunk_size: int = 500, chunk_overlap: int = 100):
        """ä»æ–‡ä»¶æ„å»ºå‘é‡ç´¢å¼•"""
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        self.build_from_text(text, chunk_size, chunk_overlap)
    
    def _split_text(self, text: str, chunk_size: int, chunk_overlap: int) -> List[TextChunk]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        
        # æŒ‰ç« èŠ‚åˆ†å‰²
        import re
        sections = re.split(r'\n===\s*(.*?)\s*===\n', text)
        
        current_chapter = "æœªçŸ¥ç« èŠ‚"
        for i, section in enumerate(sections):
            if i % 2 == 1:
                # è¿™æ˜¯ç« èŠ‚æ ‡é¢˜
                current_chapter = section
                continue
            
            section = section.strip()
            if len(section) < 50:
                continue
            
            # æŒ‰æ®µè½è¿›ä¸€æ­¥åˆ†å‰²
            paragraphs = section.split('\n\n')
            current_chunk = ""
            
            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue
                
                if len(current_chunk) + len(para) < chunk_size:
                    current_chunk += para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(TextChunk(
                            text=current_chunk.strip(),
                            chapter=current_chapter,
                            index=len(chunks)
                        ))
                    # ä¿ç•™é‡å éƒ¨åˆ†
                    if chunk_overlap > 0 and len(current_chunk) > chunk_overlap:
                        current_chunk = current_chunk[-chunk_overlap:] + para + "\n\n"
                    else:
                        current_chunk = para + "\n\n"
            
            if current_chunk.strip():
                chunks.append(TextChunk(
                    text=current_chunk.strip(),
                    chapter=current_chapter,
                    index=len(chunks)
                ))
        
        return chunks
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[TextChunk, float]]:
        """æœç´¢æœ€ç›¸å…³çš„æ–‡æœ¬å—"""
        if self.embeddings is None or len(self.chunks) == 0:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self._embed_texts([query])[0]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        # å½’ä¸€åŒ–
        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-9)
        embeddings_norm = self.embeddings / (np.linalg.norm(self.embeddings, axis=1, keepdims=True) + 1e-9)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(embeddings_norm, query_norm)
        
        # è·å– top_k ç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((self.chunks[idx], float(similarities[idx])))
        
        return results
    
    def save(self, output_dir: str):
        """ä¿å­˜å‘é‡ç´¢å¼•"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ chunks
        chunks_data = [
            {"text": c.text, "chapter": c.chapter, "index": c.index}
            for c in self.chunks
        ]
        with open(output_dir / "chunks.json", 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ embeddings
        if self.embeddings is not None:
            np.save(output_dir / "embeddings.npy", self.embeddings)
        
        console.print(f"[green]âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜åˆ° {output_dir}[/green]")
    
    def load(self, input_dir: str) -> bool:
        """åŠ è½½å‘é‡ç´¢å¼•"""
        input_dir = Path(input_dir)
        
        chunks_path = input_dir / "chunks.json"
        embeddings_path = input_dir / "embeddings.npy"
        
        if not chunks_path.exists() or not embeddings_path.exists():
            return False
        
        # åŠ è½½ chunks
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        self.chunks = [
            TextChunk(text=c["text"], chapter=c["chapter"], index=c["index"])
            for c in chunks_data
        ]
        
        # åŠ è½½ embeddings
        self.embeddings = np.load(embeddings_path)
        
        console.print(f"[green]âœ… å·²åŠ è½½å‘é‡ç´¢å¼•: {len(self.chunks)} ä¸ªæ–‡æœ¬å—[/green]")
        return True


class HybridQueryEngine:
    """æ··åˆæŸ¥è¯¢å¼•æ“ï¼šç»“åˆå‘é‡æ£€ç´¢ + å›¾è°±æ£€ç´¢ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰"""
    
    def __init__(self, vector_store: VectorStore, graph, llm):
        self.vector_store = vector_store
        self.graph = graph
        self.llm = llm
    
    def query(self, question: str, top_k: int = 5, history: List[Dict] = None) -> Dict:
        """å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ
        
        Args:
            question: å½“å‰é—®é¢˜
            top_k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            history: å¯¹è¯å†å² [{"role": "user"/"assistant", "content": "..."}]
        """
        console.print(f"\n[bold cyan]é—®é¢˜:[/bold cyan] {question}")
        if history:
            console.print(f"[dim]ğŸ“œ å¯¹è¯å†å²: {len(history)} è½®[/dim]")
        
        # 1. å‘é‡æ£€ç´¢ï¼šä»ä¹¦ä¸­æ‰¾åˆ°ç›¸å…³æ®µè½ï¼ˆä¹Ÿè€ƒè™‘å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        console.print("[dim]ğŸ” ä»ä¹¦ä¸­æ£€ç´¢ç›¸å…³æ®µè½...[/dim]")
        # ç»“åˆå†å²é—®é¢˜è¿›è¡Œæ£€ç´¢
        search_query = self._build_search_query(question, history)
        vector_results = self.vector_store.search(search_query, top_k=top_k)
        
        # 2. å›¾è°±æ£€ç´¢ï¼šæ‰¾åˆ°ç›¸å…³å®ä½“å’Œå…³ç³»
        console.print("[dim]ğŸ”— ä»çŸ¥è¯†å›¾è°±æ£€ç´¢...[/dim]")
        graph_context = self._get_graph_context(question)
        
        # 3. ç»„åˆä¸Šä¸‹æ–‡
        context = self._build_context(vector_results, graph_context)
        
        # 4. ç”Ÿæˆå›ç­”ï¼ˆä¼ å…¥å¯¹è¯å†å²ï¼‰
        answer = self._generate_answer(question, context, vector_results, history)
        
        # 5. æå–å¼•ç”¨ä¿¡æ¯
        citations = self._extract_citations(vector_results)
        
        return {
            "answer": answer,
            "citations": citations,
            "graph_entities": self._get_matched_entities(question)
        }
    
    def _build_search_query(self, question: str, history: List[Dict] = None) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆç»“åˆå¯¹è¯å†å²ï¼‰"""
        if not history:
            return question
        
        # æå–æœ€è¿‘2è½®å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯
        recent_context = []
        for msg in history[-4:]:  # æœ€è¿‘2è½®ï¼ˆ4æ¡æ¶ˆæ¯ï¼‰
            if msg.get("role") == "user":
                recent_context.append(msg.get("content", "")[:100])
        
        # ç»„åˆæŸ¥è¯¢
        if recent_context:
            return f"{' '.join(recent_context)} {question}"
        return question
    
    def _get_matched_entities(self, question: str) -> List[Dict]:
        """è·å–åŒ¹é…çš„å®ä½“åˆ—è¡¨"""
        if self.graph is None:
            return []
        
        matched = []
        for name, entity in self.graph.entities.items():
            if any(char in name for char in question if char not in "ï¼Ÿ?çš„æ˜¯ä»€ä¹ˆæ€ä¹ˆå¦‚ä½•ä¸º"):
                matched.append({
                    "name": name,
                    "type": entity.entity_type.value,
                    "description": entity.description
                })
        return matched[:5]
    
    def _extract_citations(self, vector_results: List[Tuple[TextChunk, float]]) -> List[Dict]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for i, (chunk, score) in enumerate(vector_results):
            if score > 0.3:
                citations.append({
                    "id": i + 1,
                    "chapter": chunk.chapter,
                    "text": chunk.text[:300] + "..." if len(chunk.text) > 300 else chunk.text,
                    "score": round(score, 3)
                })
        return citations
    
    def _get_graph_context(self, question: str) -> str:
        """ä»å›¾è°±è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        if self.graph is None:
            return ""
        
        context_parts = []
        
        # åœ¨å®ä½“ä¸­æœç´¢
        for name, entity in self.graph.entities.items():
            # ç®€å•åŒ¹é…
            if any(char in name for char in question if char not in "ï¼Ÿ?çš„æ˜¯ä»€ä¹ˆæ€ä¹ˆå¦‚ä½•ä¸º"):
                context_parts.append(f"- **{name}**ï¼ˆ{entity.entity_type.value}ï¼‰ï¼š{entity.description}")
                
                # è·å–å…³ç³»
                neighbors = self.graph.get_neighbors(name)
                if neighbors.get("out"):
                    for rel in neighbors["out"][:3]:
                        context_parts.append(f"  â†’ {rel['relation']} â†’ {rel['entity']}")
        
        return "\n".join(context_parts[:10])
    
    def _build_context(self, vector_results: List[Tuple[TextChunk, float]], graph_context: str) -> str:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        parts = []
        
        # æ·»åŠ ä¹¦ä¸­åŸæ–‡ï¼ˆå¸¦ç¼–å·ï¼Œä¾¿äºå¼•ç”¨ï¼‰
        if vector_results:
            parts.append("## ğŸ“– ä¹¦ä¸­ç›¸å…³åŸæ–‡\n")
            for i, (chunk, score) in enumerate(vector_results):
                if score > 0.3:
                    parts.append(f"**[{i+1}] æ¥æºï¼š{chunk.chapter}**")
                    parts.append(f"> {chunk.text[:500]}...")
                    parts.append("")
        
        # æ·»åŠ å›¾è°±ä¿¡æ¯
        if graph_context:
            parts.append("\n## ğŸ”— çŸ¥è¯†å›¾è°±ä¸­çš„ç›¸å…³æ¦‚å¿µ\n")
            parts.append(graph_context)
        
        return "\n".join(parts)
    
    def _generate_answer(self, question: str, context: str, vector_results: List[Tuple[TextChunk, float]], history: List[Dict] = None) -> str:
        """ç”Ÿæˆå¸¦å¼•ç”¨çš„å›ç­”ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰"""
        
        # æ„å»ºå¯¹è¯å†å²éƒ¨åˆ†
        history_text = ""
        if history and len(history) > 0:
            history_text = "\n## ğŸ“œ å¯¹è¯å†å²\n"
            # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
            recent_history = history[-6:]
            for msg in recent_history:
                role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
                content = msg.get("content", "")[:300]  # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                # æ¸…ç† HTML æ ‡ç­¾
                import re
                content = re.sub(r'<[^>]+>', '', content)
                history_text += f"**{role}**: {content}\n\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šæŸ¥ç†Â·èŠ’æ ¼æ€æƒ³çš„ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
{history_text}
## å‚è€ƒä¿¡æ¯
{context}

## å½“å‰é—®é¢˜
{question}

## å›ç­”è¦æ±‚
1. **æ³¨æ„å¯¹è¯ä¸Šä¸‹æ–‡**ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠåˆ°ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·æ„å›¾
2. **å¿…é¡»å¼•ç”¨ä¹¦ä¸­åŸæ–‡**ï¼šåœ¨å¼•ç”¨æ—¶ä½¿ç”¨ [1]ã€[2] ç­‰æ ‡è®°å¯¹åº”ä¸Šé¢çš„æ¥æºç¼–å·
3. ä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡å›ç­”ï¼ŒåŒ…æ‹¬ï¼š
   - ä½¿ç”¨ `>` å¼•ç”¨ä¹¦ä¸­çš„é‡è¦åŸè¯
   - ä½¿ç”¨ `**ç²—ä½“**` å¼ºè°ƒå…³é”®æ¦‚å¿µ
   - ä½¿ç”¨åˆ—è¡¨ç»„ç»‡è¦ç‚¹
4. ç»“åˆçŸ¥è¯†å›¾è°±ä¸­çš„å…³ç³»è¿›è¡Œæ·±åº¦åˆ†æ
5. å›ç­”è¦å‡†ç¡®ã€æœ‰æ·±åº¦ã€æœ‰æ¡ç†
6. å¦‚æœç”¨æˆ·åœ¨è¿½é—®æˆ–è¦æ±‚å±•å¼€ï¼Œè¯·åŸºäºä¹‹å‰çš„å›ç­”è¿›è¡Œæ·±å…¥
7. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

        try:
            if hasattr(self.llm, 'complete'):
                response = self.llm.complete(prompt)
                if hasattr(response, 'text'):
                    return response.text
                return str(response)
            else:
                return self.llm.chat([{"role": "user", "content": prompt}])
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"

